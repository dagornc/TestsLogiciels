# IntelliTest-Architect ü§ñ

**IntelliTest-Architect** est un syst√®me backend intelligent qui automatise la g√©n√©ration d'artefacts de test logiciel en utilisant la puissance des grands mod√®les de langage (LLM). Il analyse votre code source et produit des plans de test, des sc√©narios d'int√©gration et des tests unitaires, vous faisant gagner un temps pr√©cieux et am√©liorant la qualit√© de vos projets.

---

### ‚ú® Description pour les D√©butants

Imaginez que vous ayez un assistant expert en tests qui peut lire votre code et √©crire tous les documents de test ennuyeux pour vous. C'est exactement ce que fait **IntelliTest-Architect** ! Vous lui donnez le chemin de votre projet, vous choisissez le "cerveau" (un mod√®le IA comme Gemma ou Llama) qu'il doit utiliser, et il g√©n√®re automatiquement des documents clairs qui vous expliquent comment tester votre application. C'est un gain de temps √©norme et un excellent moyen d'apprendre les bonnes pratiques de test !

---

### üöÄ Description Technique pour les Experts

**IntelliTest-Architect** est un outil d'ing√©nierie logicielle assist√©e par IA, con√ßu pour s'int√©grer dans des flux de travail de d√©veloppement modernes. Il est √©crit en Python 3.12 et enti√®rement conteneuris√© avec Docker pour une portabilit√© et une reproductibilit√© maximales.

**Architecture :**
Le syst√®me est construit sur une architecture modulaire et d√©coupl√©e, facilitant son extension et sa maintenance.
* **`ConfigManager` (`config_manager.py`) :** Un singleton responsable du chargement, de la validation et de la fourniture de la configuration √† partir du fichier `config.yaml`. Il assure que l'ensemble de l'application partage une configuration unique et coh√©rente.
* **`LLMHandler` (`llm_handler.py`) :** Le c≈ìur de l'interaction avec l'IA. Cette classe abstraite la complexit√© de la communication avec diff√©rents fournisseurs de LLM. En utilisant un design pattern "Strategy", elle peut basculer de mani√®re transparente entre :
    * **Ollama :** Pour les mod√®les auto-h√©berg√©s, via des requ√™tes HTTP simples.
    * **Hugging Face :** Pour l'ex√©cution locale de mod√®les open-source (comme Gemma, Mistral) via la biblioth√®que `transformers` de Hugging Face et `torch`. Elle g√®re le chargement du mod√®le et du tokenizer.
    * **API Externe :** Pour les services compatibles avec l'API OpenAI (OpenRouter, Perplexity, etc.), en utilisant des requ√™tes HTTP authentifi√©es.
* **`TestGenerator` (`test_generator.py`) :** Contient la logique m√©tier principale. Il orchestre le processus :
    1.  Lecture de la structure du projet cible.
    2.  It√©ration sur les documents √† g√©n√©rer d√©finis dans la configuration.
    3.  Construction de prompts sp√©cifiques pour chaque t√¢che.
    4.  Invocation du `LLMHandler` pour obtenir les g√©n√©rations.
    5.  Sauvegarde des artefacts produits dans le r√©pertoire de sortie.
* **`FileHandler` (`file_handler.py`) :** Fournit des utilitaires robustes pour toutes les op√©rations sur les fichiers (lecture, √©criture, cr√©ation de sauvegardes ZIP), en utilisant `pathlib` pour une gestion moderne des chemins.
* **`main.py` :** Le point d'entr√©e qui initialise les services (logger, config) et lance le `TestGenerator`.

**Flux d'Ex√©cution :**
1.  Au lancement, `main.py` initialise le logger et charge la configuration.
2.  Le `TestGenerator` est instanci√© avec la configuration charg√©e.
3.  Il analyse le `project_path` pour rassembler le contexte du code source.
4.  Pour chaque document activ√© dans `documents_to_generate`, il formate un prompt d√©taill√© incluant le contexte du code et la t√¢che demand√©e.
5.  Le prompt est envoy√© au `LLMHandler` qui, en fonction du `llm_provider` configur√©, interagit avec le LLM appropri√©.
6.  La r√©ponse du LLM est re√ßue, nettoy√©e et sauvegard√©e dans un fichier Markdown dans le r√©pertoire de sortie.
7.  Des logs d√©taill√©s sont √©mis √† chaque √©tape pour un monitoring complet.

### üõ†Ô∏è Installation et Utilisation

**Pr√©requis :**
* Docker
* (Optionnel) Ollama si vous souhaitez utiliser un mod√®le local.

**Lancement :**
1.  **Configurez :** Modifiez le fichier `config.yaml` pour pointer vers votre projet (`project_path`) et choisir votre fournisseur de LLM.
2.  **Ex√©cutez le script :** Lancez le script `run.sh` :
    ```bash
    bash run.sh
    ```
    Ce script va automatiquement :
    * Construire l'image Docker `intellitest-architect`.
    * Lancer un conteneur en montant votre r√©pertoire `/root/projets` pour que l'application puisse y acc√©der.
3.  **V√©rifiez les r√©sultats :** Les documents de test g√©n√©r√©s appara√Ætront dans le sous-r√©pertoire `tests_generes` de votre projet. Les logs seront disponibles dans `intellitest.log`.
